---
title: "Bayesian Linear Regression"
author: "Remy"
date: "9/16/2020"
output: 
  pdf_document
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{setspace}\doublespacing
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction to linear regression

Linear regression is a statistical model that assumes a linear relationship between a numerical output variable $y$, commonly called the response variable,  and at least one of the $p$ input variables $\mathbf{x_1, x_2, \ldots, x_p}$, commonly known as predictors. Assuming a linear relationship between the response and the predictor(s) allows us to formulate the model by writing the response variable as a linear combination of the predictor(s) as shown in Equation \ref{eqn:l.Reg1}.

Fitting a linear regression is training the model on the observed data to learn about the linear relationship between the response and the predictor variables, in order to identify potential predictors that constitute the regression equation of the best fit. Potential predictors are the ones that have significant linear relationship with the response. In practice, the significance of a predictor variable, $\mathbf{x}_j$ with $j = 1, \ldots, p$, is determined by the value of its corresponding coefficient $\beta_j$ as shown in the Equation \ref{eqn:l.Reg1} below:  

\begin{equation}
\label{eqn:l.Reg1}
\mathbf{y} = \beta_0 + \beta_1\mathbf{x_1} + \beta_2\mathbf{x_2} + \ldots + \beta_p\mathbf{x_p} + \boldsymbol{\epsilon},
\end{equation}

which is equivalent to

\begin{equation}
\label{eqn:l.Reg2}
\mathbf{y} = X\boldsymbol{\beta + \epsilon}
\end{equation}

 with $\boldsymbol{\epsilon} \sim N(0,\sigma^2I_n)$.


The unknown parameters in the Equation \ref{eqn:l.Reg1}, which are the regression coefficients $\boldsymbol{\beta} = (\beta_1, \beta_2, \ldots, \beta_p)'$ and the variance of the error term, $\sigma^2$, can be estimated either by frequentest methods, the most common for linear regression being least square methods, or in Bayesian framework. In frequentest models, parameters are assumed to be unknown, but with fixed values, while in Bayesian inference parameters are modeled as random variables through probability distributions. For the rest of this post, I am going to give a brief introduction on Bayesian inference and then focus on how to build linear regression models in Bayesian framework.

# Bayesian inference

Bayesian inference is generally implemented in two main steps. First, we start with  possible prior knowledge about model parameters: prior distributions; and second, given that we have observed new data, we update the prior using Bayes theorem to get the posterior distributions as follows:

\begin{equation}\label{eqn:Bayesian_theorm}
[\boldsymbol{\theta}|X] = \frac{[X |\boldsymbol{\theta}][\boldsymbol{\theta}]}{[X]} \propto [X |\boldsymbol{\theta}][\boldsymbol{\theta}],
\end{equation}

where $\boldsymbol{\theta} = (\boldsymbol{\beta}, \sigma^2)'$.

There are two common issues in Bayesian inference related to the Bayesian theorem presented in Equation \ref{eqn:Bayesian_theorm}: One is that the marginal distribution of the data, [X] is not easy to compute, especially in high dimensional data, and the other one is that in the case of non conjugate prior, the posterior 
$[X |\boldsymbol{\theta}]$ is generally intractable. An intractable distribution is the one that cannot be evaluated analytically and therefore, requires special computation methods.

To overcome these computational challenges, in Bayesian inference the posterior distribution is estimated by using Markov Chain Monte Carlo (MCMC) methods, which allows us to estimate the posterior distribution by sampling from it. MCMC allows us to estimated the posterior distribution by constructing a Markov Chain that has the target posterior distribution as its limiting distribution.
Nowadays, there are various methods to implement MCMC, but  the most common are Gibbs sampler (GS) and Metropolis Hastings (MH) methods. The Gibbs Sampler is used when the full conditional distributions are analytically known, while MH is commonly used when the full conditional posterior distribution of parameters cannot be calculated analytically, which is generally the case for non conjugate priors. 

In practice, the main difference between the two is related to their acceptance probability during MCMC iterations. 
For each MCMC iteration in MH, a new random value for model parameters is proposed. The proposed value is sampled from a proposal distribution, which is usual a normal distribution, and accepted with some acceptance probability $\alpha < 1$. On the other hand, in GS the acceptance probability is always equal to 1 because the new values is sample from the full conditional distributions rather than the proposal distributions as in MH.

## Gibbs Sampler algorithm 

Suppose we have: \
* A vector of parameters $\boldsymbol{\theta} = (\theta_1,\ldots\,\theta_d)'$, which in the case of linear regression is equivalent to $(\beta_1, \beta_2,\ldots\,\beta_p, \sigma^2)'$, and

* A full conditional posterior for each parameter $[\theta_j|\{\theta_k, k\neq j\}, X]$. Now, 

* Initialize $\boldsymbol{\boldsymbol{\theta}}$, to $(\theta_1^0,\ldots,\theta_d^0)'$,

* Then for $t = 1,\ldots, N$, sample:
\begin{align*}
&\theta_1^{(t)} && from && [\theta_1^{(t)}|\{\theta_j^{(t-1)},j\neq 1\}, X]\\
&\theta_2^{(t)} && from  && [\theta_2^{(t)}|\theta_1^{(t)},\{\theta_j^{(t-1)},j>2\}, X]\\
&\theta_3^{(t)} && from && [\theta_3^{(t)}|\theta_1^{(t)},\theta_2^{(t)} ,\{\theta_j^{(t-1)},j>3\}, X]\\
&\vdots  && \vdots          &&    \vdots \\
&\theta_d^{(t)} && from && [\theta_d^{(t)}|\{\theta_j^{(t)},j<d\}, X],
\end{align*}

with $N$, the number of MCMC iterations, $X$ the data matrix, and $d$ the number of model parameters.
As a result, as time $t\rightarrow\infty$, the joint distribution of a sample $\boldsymbol{\theta}^{(t)} = \theta_1^{(t)},\ldots,\theta_d^{(t)}$ converges to the true joint distribution of $\boldsymbol{\theta}$ 
(Gelfand & Smith 1990).

## Metropolis hastings algorithm 

* Suppose we have our target distribution $[\boldsymbol{\theta} | .]$, and the the proposal distribution $[\boldsymbol{\theta^*|\theta}]$.


* Let $\boldsymbol{\theta}^{(t)}$ be the current value of $\boldsymbol{\theta}$, and $\boldsymbol{\theta}^{(t+1)}$ the new value of $\boldsymbol{\theta}$

Then initialize $\boldsymbol{\theta}$ to $\boldsymbol{\theta}^{(0)}$, and\

For $t = 1,\ldots,N$, do:\
propose $\boldsymbol{\theta}^*\sim [\boldsymbol{\theta}^* | \boldsymbol{\theta}^{(t)}]$\
Compute the acceptance probability:
\[
 \alpha = min\left(1,\frac{[\boldsymbol{\theta}^* | .]}{[\boldsymbol{\theta}^{(t)} | .]}\times
 \frac{[\boldsymbol{\theta}^{(t)} |\boldsymbol{\theta}^*]}{[\boldsymbol{\theta}^*|\boldsymbol{\theta}^{(t)} ]}\right)
\]
Sample  $u\sim Unif[0,1]$\
If $u < \alpha$, set $\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^*$\
otherwise, set $\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)}$\
End


# Gibbs sampler in linear regression
## Likelihood function and Data distribution

Referring to Equation \ref{eqn:l.Reg2}, where $X\boldsymbol{\beta}$ represents the mean of $\mathbf{y}$ and $\boldsymbol{\epsilon}$ the error term, we can deduce that 

\begin{equation}
\label{eqn:l.Reg3}
\mathbf{y} \sim N(X\boldsymbol{\beta}, \sigma^2I_n).
\end{equation}

Therefore, the distribution of the date is a multivariate normal with density

\begin{equation}
\label{eqn:l.Reg3}
[\mathbf{y}|\boldsymbol{\beta}, \sigma^2] \propto (\sigma^2)^{-n/2}\exp \left\{(\mathbf{y} - X\boldsymbol{\beta)}^T\Sigma^{-1}(\mathbf{y}-X\boldsymbol{\beta)} \right\}
\end{equation}


with $\Sigma = \sigma^2I_n$

## Prior distribution

There are different choices for prior distributions for parameter $\boldsymbol{\beta} = (\beta_1, \beta_2, \ldots, \beta_p)'$ and $\sigma^2$, but for computational efficiency, in this post we prefer to use conjugate priors. Conjugate priors allows us to calculate posterior distributions analytically, and hence, with the knowledge of the full conditional distributions we can apply Gibbs sampling.

For the normal likelihood, one possible conjugate prior for $\sigma^2$ in an inverse gamma, while $\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_p)'$ can be assigned a normal prior or a Non-informative prior, such us a uniform prior. In this post, we are considering an inverse gamma prior for $\sigma^2$, $\sigma^2 \sim IG(a, b)$ and a normal prior for $\boldsymbol{\beta}$, $\boldsymbol{\beta}\sim N(\boldsymbol{\mu_0, \Sigma_0})$, where $\boldsymbol{\mu_0} = (\mu_{01}, \ldots, \mu_{0p})'$ is a prior mean vector and $\Sigma_0$ a prior covariance matrix.

## Normal distribution for regression coefficints
Why did we choose a normal distribution for the regression coefficients? \
Recall that the Least Square Estimate for $\boldsymbol{\beta}$, $\boldsymbol{\hat{\beta}}$ is given by 

\begin{equation}
\label{eqn:beta-hat}
\boldsymbol{\hat{\beta} = (X^TX)^{-1}X^Ty},
\end{equation}

and 

\begin{equation}
\label{eqn:var-beta-hat}
var(\boldsymbol{\hat{\beta}}) = \sigma^2(X^{T}X)^{-1}.
\end{equation}


Notice that $\boldsymbol{\hat{\beta} = \underbrace{(X^TX)^{-1}X^T}_{constant}y}$
is a linear combination of multivariate normal random variables, $\mathbf{y}$, which leads to conclude that $\boldsymbol{\hat{\beta}}$ will also have a normal distribution.
Therefore, from the Equation \ref{eqn:beta-hat} and \ref{eqn:var-beta-hat} we can estimated the distribution for the regression coefficients $\boldsymbol{\beta} = (\beta_1,\ldots\,\beta_p)'$ as 
$$\boldsymbol{\beta \sim N(\hat{\beta}, \sigma^2(X^{T}X)^{-1})}$$.
Hence, for posterior updates, it is reasonable to assign a normal prior to $\boldsymbol{\beta}$.

## Posterior distribution
The posterior distribution is the distribution of the parameters after we observe new data.
Before we observe any data, the distributions of the parameters (priors) are just our best guesses.
When we observed new data, we use Bayesian theorem in Equation \ref{eqn:Bayesian_theorm} to update the priors, based on the data we just collected. The updated distribution, is our posterior distribution.
In practice, the more data we observe, the better we can estimate the true distribution of the parameters.

### Posterior distribution for $\boldsymbol{\beta}$ 

Given the observed data $\mathbf{y|X}, \boldsymbol{\beta},\sigma^2 \sim N(X\boldsymbol{\beta}, \sigma^2I_n)$, and the prior on $\boldsymbol{\beta}$ being $N(\boldsymbol{\mu_0, \Sigma_0})$,
the posterior distribution of $\boldsymbol{\beta}$, \
$\boldsymbol{\beta| .}\sim N(\mu_0, \Sigma_0) \times N(\boldsymbol{X\beta, \sigma^2I_n}) \sim N(\boldsymbol{\mu_n, \Sigma_n})$,
with

\begin{equation}
\label{eqn:post_var}
 \boldsymbol{\Sigma_n} = \left (\boldsymbol{\Sigma_0^{-1} + (\sigma^2)^{-1}(X^{T}X)} \right)^{-1}
\end{equation}

\begin{equation}
\label{eqn:post_mean}
\boldsymbol{\mu_n = \Sigma_n} \left (\boldsymbol{\Sigma_0^{-1}\mu_0 + (\sigma^2)^{-1}(X^{T}X)\hat{\beta}} \right)^{-1}
\end{equation}

In practice, it is common to choose a prior for $\boldsymbol{\beta }$ to be $N(\boldsymbol{0,\Sigma_0})$, which simplifies the expression in Equation \ref{eqn:post_mean} to $$\boldsymbol{\mu_n} = \boldsymbol{\Sigma_n} \left (\frac{1}{\sigma^2}\boldsymbol{X^Ty} \right)^{-1}.$$

### Posterior distribution for $\sigma^2$

Given $\boldsymbol{\beta}$, the posterior for $\sigma^2$ can be calculated as follows:\
$\sigma^2|\boldsymbol{y,\beta} \sim N(\boldsymbol{X\beta, \sigma^2I_n}) \times IG(a , b) \sim IG(a_n , b_n)$, where

* $a_n = \frac{n}{2} + a$ and
* $b_n = \frac{1}{2\sigma^2}\boldsymbol{(y - X\beta)^T(y -  X\beta)} + b$


## Implementing Gibbs Sampler for linear regression in R








