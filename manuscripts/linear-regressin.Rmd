---
title: "Bayesian Linear Regression"
author: "Remy"
date: "9/16/2020"
output:  
  bookdown::pdf_document2:
     keep_tex: true
     toc: true
  
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{setspace}\doublespacing
---

\newpage

# Introduction to linear regression

Linear regression is a statistical model that assumes a linear relationship between a numerical output variable $y$, commonly called the response variable,  and at least one of the $p$ input variables $\mathbf{x_1, x_2, \ldots, x_p}$, commonly known as predictors. Assuming a linear relationship between the response and the predictor(s) allows us to formulate the model by writing the response variable as a linear combination of the predictor(s) as shown in Equation \ref{eqn:l.Reg1}.

Fitting a linear regression is training the model on the observed data to learn about the linear relationship between the response and the predictor variables, in order to identify potential predictors that constitute the regression equation of the best fit. Potential predictors are the ones that have significant linear relationship with the response. In practice, the significance of a predictor variable, $\mathbf{x}_j$ with $j = 1, \ldots, p$, is determined by the value of its corresponding coefficient $\beta_j$ as shown in the Equation \ref{eqn:l.Reg1} below:  

\begin{equation}
\label{eqn:l.Reg1}
\mathbf{y} = \beta_0 + \beta_1\mathbf{x_1} + \beta_2\mathbf{x_2} + \ldots + \beta_p\mathbf{x_p} + \boldsymbol{\epsilon},
\end{equation}

which is equivalent to

\begin{equation}
\label{eqn:l.Reg2}
\mathbf{y} = X\boldsymbol{\beta + \epsilon}
\end{equation}

 with $\boldsymbol{\epsilon} \sim N(0,\sigma^2I_n)$.


The unknown parameters in Equation \ref{eqn:l.Reg1}, which are the regression coefficients $\boldsymbol{\beta} = (\beta_1, \beta_2, \ldots, \beta_p)'$ and the variance of the error term, $\sigma^2$, can be estimated either by frequentist methods, the most common for linear regression being least square methods, or in Bayesian framework. In frequentist models, parameters are assumed to be unknown, but with fixed values, while in Bayesian inference parameters are modeled as random variables through probability distributions. For the rest of this post, I am going to give a brief introduction to Bayesian inference and then focus on how to build linear regression models in the Bayesian framework.

# Bayesian inference

Bayesian inference is generally implemented in two main steps. First, we start with  possible prior knowledge about model parameters: prior distributions; and second, given that we have observed new data, we update the prior using Bayes theorem to get the posterior distributions as follows:

\begin{equation}\label{eqn:Bayesian_theorm}
[\boldsymbol{\theta}|X] = \frac{[X |\boldsymbol{\theta}][\boldsymbol{\theta}]}{[X]} \propto [X |\boldsymbol{\theta}][\boldsymbol{\theta}],
\end{equation}

where $\boldsymbol{\theta} = (\boldsymbol{\beta}, \sigma^2)'$.

There are two common issues in Bayesian inference related to the Bayesian theorem presented in Equation \ref{eqn:Bayesian_theorm}: One is that the marginal distribution of the data, [X] is not easy to compute, especially in high dimensional data, and the other one is that in the case of non-conjugate prior, the posterior $[X |\boldsymbol{\theta}]$ is generally intractable. An intractable distribution is the one that cannot be evaluated analytically and therefore, requires special computation methods.

To overcome these computational challenges, in Bayesian inference the posterior distribution is estimated by using Markov Chain Monte Carlo (MCMC) methods, which allows us to estimate the posterior distribution by sampling from it. MCMC allows us to estimate the posterior distribution by constructing a Markov Chain that has the target posterior distribution as its limiting distribution.
Nowadays, there are various methods to implement MCMC, but the most common is the Gibbs sampler (GS) and Metropolis-Hastings (MH) methods. The Gibbs Sampler is used when the full conditional distributions are analytically known, while MH is commonly used when the full conditional posterior distributions of parameters cannot be calculated analytically, which is generally the case for non-conjugate priors. 

In practice, the main difference between the two is related to their acceptance probability during MCMC iterations. 
For each MCMC iteration in MH, a new random value for model parameters is proposed. The proposed value is sampled from a proposal distribution, which is usually a normal distribution, and accepted with some acceptance probability $\alpha < 1$. On the other hand, in GS the acceptance probability is always equal to 1 because the new values are samples from the full conditional distributions rather than the proposal distributions as in MH.

## Gibbs Sampling algorithm 

Suppose we have: \
* A vector of parameters $\boldsymbol{\theta} = (\theta_1,\ldots\,\theta_d)'$, which in the case of linear regression is equivalent to $(\beta_1, \beta_2,\ldots\,\beta_p, \sigma^2)'$, and

* A full conditional posterior for each parameter $[\theta_j|\{\theta_k, k\neq j\}, X]$. Now, 

* Initialize $\boldsymbol{\boldsymbol{\theta}}$, to $(\theta_1^0,\ldots,\theta_d^0)'$,

* Then for $t = 1,\ldots, N$, sample:
\begin{align*}
&\theta_1^{(t)} && from && [\theta_1^{(t)}|\{\theta_j^{(t-1)},j\neq 1\}, X]\\
&\theta_2^{(t)} && from  && [\theta_2^{(t)}|\theta_1^{(t)},\{\theta_j^{(t-1)},j>2\}, X]\\
&\theta_3^{(t)} && from && [\theta_3^{(t)}|\theta_1^{(t)},\theta_2^{(t)} ,\{\theta_j^{(t-1)},j>3\}, X]\\
&\vdots  && \vdots          &&    \vdots \\
&\theta_d^{(t)} && from && [\theta_d^{(t)}|\{\theta_j^{(t)},j<d\}, X],
\end{align*}

with $N$, the number of MCMC iterations, $X$ the data matrix, and $d$ the number of model parameters.
As a result, as time $t\rightarrow\infty$, the joint distribution of a sample $\boldsymbol{\theta}^{(t)} = \theta_1^{(t)},\ldots,\theta_d^{(t)}$ converges to the true joint distribution of $\boldsymbol{\theta}$ 
(Gelfand & Smith 1990).

## Metropolis Hastings algorithm 

* Suppose we have our target distribution $[\boldsymbol{\theta} | .]$, and the the proposal distribution $[\boldsymbol{\theta^*|\theta}]$.


* Let $\boldsymbol{\theta}^{(t)}$ be the current value of $\boldsymbol{\theta}$, and $\boldsymbol{\theta}^{(t+1)}$ the new value of $\boldsymbol{\theta}$

Then initialize $\boldsymbol{\theta}$ to $\boldsymbol{\theta}^{(0)}$, and\

For $t = 1,\ldots,N$, do:\
propose $\boldsymbol{\theta}^*\sim [\boldsymbol{\theta}^* | \boldsymbol{\theta}^{(t)}]$\
Compute the acceptance probability:



\begin{equation}\label{eqn:accept-ratio}
\alpha = min\left(1,\frac{[\boldsymbol{\theta}^* | .]}{[\boldsymbol{\theta}^{(t)} | .]}\times
 \frac{[\boldsymbol{\theta}^{(t)} |\boldsymbol{\theta}^*]}{[\boldsymbol{\theta}^*|\boldsymbol{\theta}^{(t)} ]}\right),
\end{equation}



\[
 \alpha = min\left(1,\frac{[\boldsymbol{\theta}^* | .]}{[\boldsymbol{\theta}^{(t)} | .]}\times
 \frac{[\boldsymbol{\theta}^{(t)} |\boldsymbol{\theta}^*]}{[\boldsymbol{\theta}^*|\boldsymbol{\theta}^{(t)} ]}\right)
\]
Sample  $u\sim Unif[0,1]$\
If $u < \alpha$, set $\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^*$\
otherwise, set $\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)}$\
End


# Gibbs sampling in linear regression

## Likelihood function and Data distribution

Referring to Equation \ref{eqn:l.Reg2}, where $X\boldsymbol{\beta}$ represents the mean of $\mathbf{y}$ and $\boldsymbol{\epsilon}$ the error term, we can deduce that 

\begin{equation}
\label{eqn:l.Reg3}
\mathbf{y} \sim N(X\boldsymbol{\beta}, \sigma^2I_n).
\end{equation}

Therefore, the distribution of the data is a multivariate normal with density

\begin{equation}
\label{eqn:l.Reg3}
[\mathbf{y}|\boldsymbol{\beta}, \sigma^2] \propto (\sigma^2)^{-n/2}\exp \left\{(\mathbf{y} - X\boldsymbol{\beta)}^T\Sigma^{-1}(\mathbf{y}-X\boldsymbol{\beta)} \right\}
\end{equation}


with $\Sigma = \sigma^2I_n$

## Prior distribution

There are different choices for prior distributions for parameter $\boldsymbol{\beta} = (\beta_1, \beta_2, \ldots, \beta_p)'$ and $\sigma^2$, but for computational efficiency, in this post we prefer to use conjugate priors. Conjugate priors allow us to calculate posterior distributions analytically, and hence, with the knowledge of the full conditional distributions we can apply Gibbs sampling.

For the normal likelihood, one possible conjugate prior for $\sigma^2$ in an inverse gamma, while $\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_p)'$ can be assigned a normal prior or a Non-informative prior, such us a uniform prior. In this post, we are considering an inverse gamma prior for $\sigma^2$, $\sigma^2 \sim IG(a, b)$ and a normal prior for $\boldsymbol{\beta}$, $\boldsymbol{\beta}\sim N(\boldsymbol{\mu_0, \Sigma_0})$, where $\boldsymbol{\mu_0} = (\mu_{01}, \ldots, \mu_{0p})'$ is a prior mean vector and $\Sigma_0$ a prior covariance matrix.

## Prior distribution for regression coefficients

A common choice  for regression coefficients prior is a Gaussian distribution.\
Recall that the least square estimate for $\boldsymbol{\beta}$, $\boldsymbol{\hat{\beta}}$ is given by 

\begin{equation}
\label{eqn:beta-hat}
\boldsymbol{\hat{\beta} = (X^TX)^{-1}X^Ty},
\end{equation}

and 

\begin{equation}
\label{eqn:var-beta-hat}
var(\boldsymbol{\hat{\beta}}) = \sigma^2(X^{T}X)^{-1}.
\end{equation}


Notice that $\boldsymbol{\hat{\beta} = \underbrace{(X^TX)^{-1}X^T}_{constant}y}$
is a linear combination of multivariate normal random variables, $\mathbf{y}$, which leads to conclude that $\boldsymbol{\hat{\beta}}$ will also have a normal distribution.
Therefore, from the Equation \ref{eqn:beta-hat} and \ref{eqn:var-beta-hat} we can estimated the distribution for the regression coefficients $\boldsymbol{\beta} = (\beta_1,\ldots\,\beta_p)'$ as 
$$\boldsymbol{\beta \sim N(\hat{\beta}, \sigma^2(X^{T}X)^{-1})}$$.
Hence, for posterior updates, it is reasonable to assign a normal prior to $\boldsymbol{\beta}$.

## Posterior distribution

The posterior distribution is the distribution of the parameters after we observe new data.
Before we observe any data, the distributions of the parameters (priors) are just our best guesses.
When we observed new data, we use the Bayesian theorem in Equation \ref{eqn:Bayesian_theorm} to update the priors, based on the data we just collected. The updated distribution is our posterior distribution.
In practice, the more data we observe, the better we can estimate the true distribution of the parameters.

### Posterior distribution for $\boldsymbol{\beta}$ 

Given the observed data $\mathbf{y|X}, \boldsymbol{\beta},\sigma^2 \sim N(X\boldsymbol{\beta}, \sigma^2I_n)$, and the prior on $\boldsymbol{\beta}$ being $N(\boldsymbol{\mu_0, \Sigma_0})$,
the posterior distribution of $\boldsymbol{\beta}$, \
$\boldsymbol{\beta| .}\sim N(\mu_0, \Sigma_0) \times N(\boldsymbol{X\beta, \sigma^2I_n}) \sim N(\boldsymbol{\mu_n, \Sigma_n})$,
with

\begin{equation}
\label{eqn:post_var}
 \boldsymbol{\Sigma_n} = \left (\boldsymbol{\Sigma_0^{-1} + (\sigma^2)^{-1}(X^{T}X)} \right)^{-1}
\end{equation}

\begin{equation}
\label{eqn:post_mean}
\boldsymbol{\mu_n = \Sigma_n} \left (\boldsymbol{\Sigma_0^{-1}\mu_0 + (\sigma^2)^{-1}(X^{T}X)\hat{\beta}} \right)^{-1}
\end{equation}

In practice, it is common to choose a prior for $\boldsymbol{\beta }$ to be $N(\boldsymbol{0,\Sigma_0})$, which simplifies the expression in Equation \ref{eqn:post_mean} to $$\boldsymbol{\mu_n} = \boldsymbol{\Sigma_n} \left (\frac{1}{\sigma^2}\boldsymbol{X^Ty} \right)^{-1}.$$

### Posterior distribution for $\sigma^2$

Given $\boldsymbol{\beta}$, the posterior for $\sigma^2$ can be calculated as follows:\
$\sigma^2|\boldsymbol{y,\beta} \sim N(\boldsymbol{X\beta, \sigma^2I_n}) \times IG(a , b) \sim IG(a_n , b_n)$, where

* $a_n = \frac{n}{2} + a$ and
* $b_n = \frac{1}{2\sigma^2}\boldsymbol{(y - X\beta)^T(y -  X\beta)} + b$


## Implementing Gibbs Sampler for linear regression in R

```{r, echo = FALSE, warning=FALSE, message=FALSE}
# source(here::here("R", "libraries.R"))
# source(here::here("R", "convert-to-coda.R"))
# source(here::here("R", "mcmc_lm.R"))
# source(here::here("R", "mcmc_lm_mh.R"))
```



```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(MASS)
library(invgamma)
#library(ggplot2)
library(here)
library(coda)
library(tidyverse)
library(tidybayes)
library(bayesplot)
source(here::here("R", "convert-to-coda.R"))
```
# Simulated data for model training and testing

In this post we use simulated data consisting of a matrix of covariates $\mathbf{X} = (\mathbf{x_1, \ldots, \mathbf{x_p}})'$, and predetermined parameters $\boldsymbol{\beta} = (\beta_0, \ldots, \beta_p)'$ and $\sigma^2$, where p represents the number covariates including the column of ones for the intercept $\beta_0$. 
In this example we simulate the data as follows:

```{r, echo = TRUE}
set.seed(111)
k <- 5
n <- 100
mu_0 <- rep(5,k)
a <- 1
b <- 1
true_sigma_sq <- rinvgamma(a,b)
X <- cbind(1,mvrnorm(n, mu_0, diag(sqrt(true_sigma_sq), nrow = k, ncol = k)))
p = ncol(X)
mean_true_beta <- rep(0, 6)
true_beta <- mvrnorm(1, mean_true_beta, diag(4, nrow = p, ncol = p))
y <- X %*% true_beta + rnorm(n)
plot(y, X %*% true_beta, main = " response vs expected response")
```

# Implementing mcmc using Gibbs sampling algorithm

Running mcmc may take a long time depending on the the type of problem we are solving. To save us this time, we run it once and save the output in our results folder so that we will not have to rerun the mcmc, but we load the output instead.

The following piece of code checks if the mcmc output "out_lm" is not already save in the results folder, and if it is not there call the function "mcmc_lm" to run mcmc using Gibbs sampling algorithm and the save the output "out_lm"
in the results folder.
 
```{r, echo = TRUE}

N = 5000

if(!file.exists(here::here("results", "out_lm"))){
   
    ptm <- proc.time() # timing
   
    out_lm <- mcmc_lm(N) 
    save(out_lm,  file = here::here("results", "out_lm"))
    
   proc.time() - ptm
   
} 

load(here::here("results", "out_lm"))

```

# Gibbs sampling model disgnostics 

## Check mcmc performance

```{r, echo = FALSE, warning=FALSE, message=FALSE}
# burn_in <- seq(1:2000)
# beta_out <- out[[1]][-burn_in, ]
# sigma_out <- out[[2]][-burn_in]

# Rearranging the mcmc output to comfort with coda package

out_coda <- as.mcmc(convert_to_coda_single_chain(out_lm))
out_coda <- tidy_draws(out_coda) ## convert to tidybay
```

### Trace plots

A trace plot is a graph showing sampled values at each mcmc  iteration. It provides a visual evaluation about convergence and mixing of the chain. As shown in the Figure \@ref(fig:trace-plot), our chain mixes well and seems to have converged.

```{r trace-plot, fig.cap="Trace plot for model parameters", echo = FALSE, warning=FALSE}

out_coda %>%
    mcmc_trace(regex_pars = c("beta", "sigma_sq"),
              facet_args = list(nrow = 3,  ncol = 3))
out_coda %>%
    mcmc_trace(regex_pars = c("sigma_sq"))

plot(out_lm$sigma_sq, type = "l" , main = "true sigma squared vs estimated sigma squared")
abline(h = true_sigma_sq, col = "red")
matplot(out_lm$beta, type = "l",  xlab = "iterations", col = c("red",  "blue", "green", "purple", "black", "orange"), main = "true beta vs estimated beta")
abline(h = true_beta, col = c("red",  "blue", "green", "purple", "black", "orange"))
```

### Acceptance ratio (talk about it in MH)

The acceptance ratio is the probability of accepting a new candidate in mcmc iterations as a sample from the distribution of the parameters. Since new sample in Gibbs Sampling iterations are drawn directly from the full conditional posterior distributions, they are accepted with probability one. However, this is not the case in MH as we can see in Figure \@ref(fig:accept-ratio)


```{r accept-ratio, fig.cap="acceptance ratio in Gibbs sampling",  echo = FALSE, warning=FALSE}

 # tracee <- mcmc(out_coda)
 # effectiveSize(tracee )
 # (1 - rejectionRate(tracee))
 #  accept_rate <- 1 - rejectionRate(mcmc(out_coda))
 # knitr::kable(accept_rate)

```

### Effective sample size

Effective sample size (ESS) can be defined as an estimate of the number of independent draws from the posterior generated by the Markov chain runs. Therefore, the larger the ratio ESS/N, where N is the number of mcmc iterations, the more efficient the the chain will be. A very small value, commonly less that 0.1 indicates a very high autocorrelation among mcmc samples. The Figure \@ref(fig:Eff-sample-size) shows that the effective sample size for this model, where the value of the ration is indicated by a dot at the end of the line.
Note that the ESS greater than one, is commonly due to the negative autocorrelation between mcmc samples.

```{r Eff-sample-size, fig.cap="mcmc effective ratios", echo = FALSE, warning=FALSE}
 mcmc_neff(effectiveSize(out_coda) / N)   
```

### Other plots illustrating model parameter posterior distributions.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_title1 <- ggtitle("Credible intervals for model parameter")
plot_title2 <- ggtitle("Credible intervals for regression coefficints")
title3 <- ggtitle("Posterior density for regression coefficients")

out_coda %>%
    mcmc_intervals(regex_pars = c("beta", "sigma_sq")) + plot_title1
out_coda %>%
    mcmc_areas(regex_pars = c("beta"),
               prob = 0.8, # 80% intervals
               prob_outer = 0.99, # 99%
               point_est = "mean") + plot_title2

color_scheme_set("blue")
mcmc_dens(out_coda, regex_pars = c("beta")) + title3 
```
### Regression model diagnostics

```{r, echo = FALSE, warning=FALSE, message=FALSE}
beta_post_mean <- apply(out_lm$beta, 2, mean) # Posterior mean of beta

y_pred <- X %*% beta_post_mean

residuals <- y - y_pred

# Fitted density vs true density

plot(density(y), col = "red", main = "Fitted density vs observed density")
lines(density(y_pred), col = "blue")
legend("topright", legend = c("true density",  "fitted density"),
       col = c("red", "blue"), lty=1:2, cex=0.8)

# Residual and fitted values plots

par(mfrow = c(2,2))
hist(residuals, main = "residual histogram")
qqnorm(residuals, main = "residual qqplot")
abline(c(0, 1), col ="blue")
plot( y_pred, residuals, main = "resuduals vs fitted values")
plot(y, y_pred, main = "Observed values vs predicted values")
abline(c(0, 1), col ="blue") ## a line of 45 degree angle
```

# Metropoilis Hastings in linear regression

Metroplolis-Hastings algorithm aims to sample from a joint posterior distribution with unknown analytic form. To draw sample from an intractable joint posterior, at each MH iterations new samples are neither directly drawn from the joint posterior distribution nor the full conditional distributions, but from an other distribution commonly  called a proposal distribution.

At each iteration of MH new samples are not directly drawn from the distribution of the parameters; they come from a proposal distribution,  commonly chosen to be Gaussian distribution. The fact that new parameter values proposed in MH mcmc iterations are not from the distribution of the parameters, they may be accepted or rejected based on how far away they are from the target joint posterior distribution. As a result, in high dimensional parameter spaces, the acceptance rate and effective sample size are generally less in MH than Gibbs sampling with the same number of iterations. Therefore, Gibbs sampling is more often preferred over MH due to the improved computational benefits, especially in high dimensional parameter space

Proposing sample from an other distribution has advantages, weakens and challenges. The main advantage is that MH does not require the full conditional posterior distributions in their parametric form, which makes it work even if the priors are not conjugate with their posteriors.
On the other hand, at each MH iteration proposed samples are accepted with probability less that one, because they come from the proposal distribution rather than being directly drawn from parameter distributions. In addition to the acceptance rate issue, the proposal distribution parameters should be chosen carefully for the chain to mix well, and it also advisable to work on log scale to avoid computational errors.

## Proposal distribution

There are various way of choosing proposal distributions, but the most common choices are normal and uniform distributions, because their supports and shapes can match many other distributions.
Normal proposals have a special advantage, because the distribution being symmetric provide a mathematical advantages leading to a simpler form of the acceptance ratio in Equation \ref{eqn:accept-ratio} to

\begin{equation}
\label{eqn:accept-ratio-reduced}
\alpha = min\left(1,\frac{[\boldsymbol{\theta}^* | .]}{[\boldsymbol{\theta}^{(t)} | .]} \right).
\end{equation}

Therefore, a normal proposal is commonly preferred in practice.

## Acceptance ration and mixing issues

With a normal proposal distribution $[x|\mu, \sigma^2]$, the acceptance and rejection of the new sample mainly depends on the proposal variance $\sigma^2$.

 * If the proposal variance is too small, the acceptance rate will be high, because the proposed value and the current value are very close. In this case however, the algorithm takes a long time to explore the whole parameter space. In addition, successive sample are highly correlated, which reduces the effective sample size and makes the algorithm converge slowly.

 * If the proposal is too wide, the proposal is more likely to be rejected and the acceptance rate will be too small, because the algorithm makes big steps from the current state to the new one. As a result, the algorithm may stay at the same location for many iterations, which produces many similar samples. Therefore, the chain will  mot mix well and hence inefficient.
Obtain efficient samples using MH algorithm with a normal proposal, the proposal variance should neither be too small nor too high.

## Working on log scale

We know by the definition of a likelihood function that it involves a product of many probabilities that may include values very close to zero. As a result, the product may be a number very close to zero that a computer may round to zero. Therefore, working with mathematical expressions with denominator involving likelihood functions would procure a numerical errors if the likelihood is not on log scale.
Therefore, in calculating the acceptance probability in Equation \ref{eqn:accept-ratio}
it is advisable to use logarithm transformation to transform the product into the sum leading to a non-zero number.

## Implementing MH in R

```{r, echo = TRUE, message=FALSE, warning=FALSE}

niter <- 5000
beta_tune <- rep(0.01, p)
sigma2_tune <- 0.01


if(!file.exists(here::here("results", "out_lm_mh"))){
   
    ptm <- proc.time() # timing
   
    out_lm_mh <- mcmc_lm_mh(niter, beta_tune, sigma2_tune) 
    save(out_lm_mh,  file = here::here("results", "out_lm_mh"))
    
   proc.time() - ptm
   
} 

load(here::here("results", "out_lm_mh"))

```

# Metropolis Hastings model disgnostics

## Check mcmc performance

```{r, echo = FALSE, warning=FALSE, message=FALSE}
# burn_in <- seq(1:2000)
# beta_out <- out[[1]][-burn_in, ]
# sigma_out <- out[[2]][-burn_in]

# Rearranging the mcmc output to comfort with coda package

out_coda_mh <- as.mcmc(convert_to_coda_single_chain(out_lm_mh))
out_coda_mh <- tidy_draws(out_coda) ## convert to tidybay
```

### Trace plots

A trace plot is a graph showing sampled values at each mcmc  iteration. It provides a visual evaluation about convergence and mixing of the chain. As shown in the Figure \@ref(fig:trace-plot), our chain mixes well and seems to have converged.

```{r trace-plot, fig.cap="Trace plot for model parameters", echo = FALSE, warning=FALSE}

out_coda_mh %>%
    mcmc_trace(regex_pars = c("beta", "sigma2"),
              facet_args = list(nrow = 3,  ncol = 3))
out_coda_mh %>%
    mcmc_trace(regex_pars = c("sigma2"))

plot(out_lm_mh$sigma2_chain, type = "l" , main = "true sigma squared vs estimated sigma squared")
abline(h = true_sigma_sq, col = "red")
matplot(out_lm_mh$beta_chain, type = "l",  xlab = "iterations", col = c("red",  "blue", "green", "purple", "black", "orange"), main = "true beta vs estimated beta")
abline(h = true_beta, col = c("red",  "blue", "green", "purple", "black", "orange"))
```

### Acceptance ratio 

```{r accept-ratio, fig.cap="acceptance ratio in MH sampling",  echo = FALSE, warning=FALSE}

acceptance <- out_lm_mh$a
accepted_samples <- length(acceptance[acceptance[TRUE]])
head(accepted_samples)

accept_ratio <- 100*(accepted_samples/niter)
accept_ratio

# knitr::kable(head(accepted_samples))
```

### Effective sample size

```{r Eff-sample-size, fig.cap="mcmc effective ratios", echo = FALSE, warning=FALSE}
 mcmc_neff(effectiveSize(out_coda_mh) / niter)   
```

### Other plots illustrating model parameter posterior distributions.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_title1 <- ggtitle("Credible intervals for model parameter")
plot_title2 <- ggtitle("Credible intervals for regression coefficints")
title3 <- ggtitle("Posterior density for regression coefficients")

out_coda_mh %>%
    mcmc_intervals(regex_pars = c("beta_chain", "sigma2_chain")) + plot_title1
out_coda_mh %>%
    mcmc_areas(regex_pars = c("beta_chain"),
               prob = 0.8, # 80% intervals
               prob_outer = 0.99, # 99%
               point_est = "mean") + plot_title2

color_scheme_set("blue")
mcmc_dens(out_coda_mh, regex_pars = c("beta_chain")) + title3 
```
### Regression model diagnostics

```{r, echo = FALSE, warning=FALSE, message=FALSE}
beta_post_mean <- apply(out_lm_mh$beta_chain, 2, mean) # Posterior mean of beta

y_pred_mh <- X %*% beta_post_mean

resid_mh <- y - y_pred_mh

# Fitted density vs true density

plot(density(y), col = "red", main = "Fitted density vs observed density")
lines(density(y_pred_mh), col = "blue")
legend("topright", legend = c("true density",  "fitted density"),
       col = c("red", "blue"), lty=1:2, cex=0.8)

# Residual and fitted values plots

par(mfrow = c(2,2))
hist(resid_mh, main = "residual histogram")
qqnorm(resid_mh, main = "residual qqplot")
abline(c(0, 1), col ="blue")
plot( y_pred_mh, resid_mh, main = "resuduals vs fitted values")
plot(y, y_pred_mh, main = "Observed values vs predicted values")
abline(c(0, 1), col ="blue") ## a line of 45 degree angle
```





