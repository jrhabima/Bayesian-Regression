---
title: "Bayesian Linear Regression"
author: "Remy"
date: "9/16/2020"
output:  
  bookdown::pdf_document2:
     keep_tex: true
     toc: true
  
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{setspace}\doublespacing
---

\newpage

# Introduction to linear regression

Linear regression is a statistical model that assumes a linear relationship between a numerical output variable $y$, commonly called the response variable,  and at least one of the $p$ input variables $\mathbf{x_1, x_2, \ldots, x_p}$, generally known as predictors, independent variables, or covariates. Assuming a linear relationship between the response and the predictor(s) allows us to formulate the model by writing the response variable as a linear combination of the predictor(s) as shown in Equation \ref{eqn:l.Reg1}; hence, a linear regression.

\begin{equation}
\label{eqn:l.Reg1}
\mathbf{y} = \beta_0 + \beta_1\mathbf{x_1} + \beta_2\mathbf{x_2} + \ldots + \beta_p\mathbf{x_p} + \boldsymbol{\epsilon},
\end{equation}

which is equivalent to

\begin{equation}
\label{eqn:l.Reg2}
\mathbf{y} = X\boldsymbol{\beta + \epsilon}
\end{equation}

with $\boldsymbol{\epsilon} \sim N(\mathbf{0},\sigma^2I_n)$.

Fitting a linear regression means training the model on the observed data in order to find regression coefficients $\boldsymbol{\beta} = (\beta_1, \beta_2, \ldots, \beta_p)'$ that best describes the linear relationship between the response and the predictor variables. The values of the regression coefficients $\beta_j$ in Equation \ref{eqn:l.Reg1} determines de strength of the linear relationship between the response variable and the predictor $\mathbf{x_j}$, which allows us to identify potential predictors to be included in the model. Therefore, reliability of a linear regression depends mainly on the accuracy in regression coefficients estimation and in selection of potential predictors.

The unknown parameters in Equation \ref{eqn:l.Reg1}, which are the regression coefficients $\boldsymbol{\beta} = (\beta_1, \beta_2, \ldots, \beta_p)'$ and the variance of the error term, $\sigma^2$, can be estimated either by frequentist methods, the most common for linear regression being least square methods, or in Bayesian framework. In frequentist models, parameters are assumed to be unknown, but with fixed values, while in Bayesian inference parameters are modeled as random variables through probability distributions. 
The rest of this post consists of two main sections: The first section gives a brief introduction to Bayesian inference, while the second section talks about Bayesian inference in linear regression models with an example using simulated data.

# Bayesian inference

Bayesian inference is generally implemented in two main steps. First, we start with possible prior knowledge about model parameters, commonly called prior distributions; and then, given that we observe new data, we update the prior distributions using Bayes theorem to get the posterior distributions as follows:

\begin{equation}\label{eqn:Bayesian_theorm}
[\boldsymbol{\theta}|X] = \frac{[X |\boldsymbol{\theta}][\boldsymbol{\theta}]}{[X]} \propto [X |\boldsymbol{\theta}][\boldsymbol{\theta}],
\end{equation}

where $\boldsymbol{\theta} = (\boldsymbol{\beta}, \sigma^2)'$.

There are two common issues in Bayesian inference related to the Bayesian theorem presented in Equation \ref{eqn:Bayesian_theorm}: One is that the marginal distribution of the data, [X] is not easy to compute, especially in high dimensional data, because it involves high dimensional integrals. The other issue  occurs in the case of non-conjugate prior, where the posterior distribution $[X |\boldsymbol{\theta}]$ is generally intractable. An intractable distribution is the one that cannot be evaluated analytically and therefore, requires special computational methods.

To overcome these computational challenges that occurs in Bayesian inference, the posterior distribution is estimated by using Markov Chain Monte Carlo (MCMC) methods. The mcmc methods are numerical sampling methods that allow us to estimate the posterior distribution by constructing a Markov Chain that has the target posterior distribution as its limiting distribution.

There are multiple algorithms that are used to implement MCMC, but in this post we focus on the two the most common methods, which are Gibbs sampling (GS) and Metropolis-Hastings (MH). In practice the choice of one over the other depends on the type of problem we dealing with. Gibbs Sampling is used when the full conditional distributions are analytically known, while MH is commonly used when the full conditional posterior distributions of parameters cannot be calculated analytically, which is generally the case for non-conjugate priors. 

For computational benefits including the mixing of the mcmc chain, effective sample size, an the cost of the algorithm, GS is usually preferred over MH.
At each MCMC iteration in MH, a new random value for model parameters is not drawn directly from the distribution of the parameters. The proposed value is sampled from a proposal distribution and accepted with some probability always less than one.
On the other hand, in GS iterations new samples are directly drawn from their full conditional distributions rather than the proposal distributions as in MH, and therefore all the samples are accepted with probability one.

## Gibbs Sampling algorithm 

Suppose we have:

* A vector of parameters $\boldsymbol{\theta} = (\theta_1,\ldots\,\theta_d)'$, which in the case of linear regression is equivalent to $(\beta_1, \beta_2,\ldots\,\beta_p, \sigma^2)'$, and

* A full conditional posterior distribution for each parameter $[\theta_j|\{\theta_k, k\neq j\}, X]$. Now, 

* Initialize $\boldsymbol{\boldsymbol{\theta}}$, to $(\theta_1^0,\ldots,\theta_d^0)'$,

* Then for $t = 1,\ldots, N$, sample:
\begin{align*}
&\theta_1^{(t)} && from && [\theta_1^{(t)}|\{\theta_j^{(t-1)},j\neq 1\}, X]\\
&\theta_2^{(t)} && from  && [\theta_2^{(t)}|\theta_1^{(t)},\{\theta_j^{(t-1)},j>2\}, X]\\
&\theta_3^{(t)} && from && [\theta_3^{(t)}|\theta_1^{(t)},\theta_2^{(t)} ,\{\theta_j^{(t-1)},j>3\}, X]\\
&\vdots  && \vdots          &&    \vdots \\
&\theta_d^{(t)} && from && [\theta_d^{(t)}|\{\theta_j^{(t)},j<d\}, X],
\end{align*}

where $N$ is the number of MCMC iterations, $X$ the data matrix, and $d$ the number of model parameters.
As a result, as time $t\rightarrow\infty$, the joint distribution of a sample $\boldsymbol{\theta}^{(t)} = \theta_1^{(t)},\ldots,\theta_d^{(t)}$ converges to the true joint distribution of $\boldsymbol{\theta}$ 
(Gelfand & Smith 1990).

## Metropolis Hastings algorithm 

* Suppose we have our target posterior distribution $[\boldsymbol{\theta} | .]$, and the the proposal distribution $[\boldsymbol{\theta^*|\theta}]$.


* Let $\boldsymbol{\theta}^{(t)}$ be the current value of $\boldsymbol{\theta}$, and $\boldsymbol{\theta}^{(t+1)}$ the new value of $\boldsymbol{\theta}$

Then initialize $\boldsymbol{\theta}$ to $\boldsymbol{\theta}^{(0)}$, and for $t = 1,\ldots,N$, do:

propose $\boldsymbol{\theta}^*\sim [\boldsymbol{\theta}^* | \boldsymbol{\theta}^{(t)}]$\
Compute the acceptance probability:

\begin{equation}\label{eqn:accept-ratio}
\alpha = min\left(1,\frac{[\boldsymbol{\theta}^* | .]}{[\boldsymbol{\theta}^{(t)} | .]}\times
 \frac{[\boldsymbol{\theta}^{(t)} |\boldsymbol{\theta}^*]}{[\boldsymbol{\theta}^*|\boldsymbol{\theta}^{(t)} ]}\right),
\end{equation}


Sample  $u\sim Unif[0,1]$\
If $u < \alpha$, set $\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^*$\
otherwise, set $\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)}$\
End


# Gibbs sampling in linear regression

## Likelihood function and Data distribution

Referring to Equation \ref{eqn:l.Reg2}, where $X\boldsymbol{\beta}$ represents the mean of $\mathbf{y}$ and $\boldsymbol{\epsilon}$ the error term, we can deduce that 

\begin{equation}
\label{eqn:l.Reg3}
\mathbf{y} \sim N(X\boldsymbol{\beta}, \sigma^2I_n).
\end{equation}

Therefore, the distribution of the data is a multivariate normal with density

\begin{equation}
\label{eqn:l.Reg3}
[\mathbf{y}|\boldsymbol{\beta}, \sigma^2] \propto (\sigma^2)^{-n/2}\exp \left\{(\mathbf{y} - X\boldsymbol{\beta)}^T\Sigma^{-1}(\mathbf{y}-X\boldsymbol{\beta)} \right\}
\end{equation}


with $\Sigma = \sigma^2I_n$

## Prior distribution

There are several choices for prior distributions for regression model parameters $\boldsymbol{\beta} = (\beta_1, \beta_2, \ldots, \beta_p)'$ and $\sigma^2$, but for computational efficiency, in this post we prefer to use conjugate priors. Conjugate priors lead to parametric full conditional posterior distributions enabling the implementation of Gibbs sampling.

For the normal likelihood, one possible conjugate prior for $\sigma^2$ in an inverse gamma, while $\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_p)'$ can be assigned a normal prior or a Non-informative prior, such us a uniform prior. In this post, we are considering an inverse gamma prior for $\sigma^2$, $\sigma^2 \sim IG(a, b)$ and a normal prior for $\boldsymbol{\beta}$, such that $\boldsymbol{\beta}\sim N(\boldsymbol{\mu_0, \Sigma_0})$, where $\boldsymbol{\mu_0} = (\mu_{01}, \ldots, \mu_{0p})'$ is a prior mean vector and $\Sigma_0$ a prior covariance matrix.

## Why a normal distribution for regression coefficients?

A common choice  for regression coefficients prior is a Gaussian distribution.\
Recall that the least square estimate for $\boldsymbol{\beta}$, $\boldsymbol{\hat{\beta}}$ is given by 

\begin{equation}
\label{eqn:beta-hat}
\boldsymbol{\hat{\beta} = (X^TX)^{-1}X^Ty},
\end{equation}

and 

\begin{equation}
\label{eqn:var-beta-hat}
var(\boldsymbol{\hat{\beta}}) = \sigma^2(X^{T}X)^{-1}.
\end{equation}


Notice that $\boldsymbol{\hat{\beta} = \underbrace{(X^TX)^{-1}X^T}_{constant}y}$
is a linear combination of multivariate normal random variables, $\mathbf{y}$, which leads to conclude that $\boldsymbol{\hat{\beta}}$ will also have a normal distribution.

Therefore, from the Equation \ref{eqn:beta-hat} and \ref{eqn:var-beta-hat} we can estimate the distribution for the regression coefficients $\boldsymbol{\beta} = (\beta_1,\ldots\,\beta_p)'$ as 
$$\boldsymbol{\beta \sim N(\hat{\beta}, \sigma^2(X^{T}X)^{-1})}.$$
Hence, for conjugate posterior update, it is reasonable to assign a normal prior to $\boldsymbol{\beta}$.

## Posterior distribution

The posterior distribution is the distribution of the parameters after we observe new data.
Before we observe any data, the distributions of the parameters (priors) are just our best guesses.
When we observed new data, we use the Bayesian theorem in Equation \ref{eqn:Bayesian_theorm} to update the priors, based on the data we just collected. The updated distribution is our posterior distribution.
In practice, the more data we observe, the better we can estimate the true distribution of the parameters.

### Posterior distribution for $\boldsymbol{\beta}$ 

Given the observed data $\mathbf{y|X}, \boldsymbol{\beta},\sigma^2 \sim N(X\boldsymbol{\beta}, \sigma^2I_n)$, and the prior on $\boldsymbol{\beta}$ being $N(\boldsymbol{\mu_0, \Sigma_0})$,
the posterior distribution of $[[\boldsymbol{\beta}$, \
$[\boldsymbol{\beta| .}]\sim N(\mu_0, \Sigma_0) \times N(\boldsymbol{X\beta, \sigma^2I_n}) \sim N(\boldsymbol{\mu_n, \Sigma_n})$,
with

\begin{equation}
\label{eqn:post_var}
 \boldsymbol{\Sigma_n} = \left (\boldsymbol{\Sigma_0^{-1} + (\sigma^2)^{-1}(X^{T}X)} \right)^{-1}
\end{equation}

\begin{equation}
\label{eqn:post_mean}
\boldsymbol{\mu_n = \Sigma_n} \left (\boldsymbol{\Sigma_0^{-1}\mu_0 + (\sigma^2)^{-1}(X^{T}X)\hat{\beta}} \right)^{-1}
\end{equation}

In practice, it is common to choose a prior for $\boldsymbol{\beta }$ to be $N(\boldsymbol{0,\Sigma_0})$, which simplifies the expression in Equation \ref{eqn:post_mean} to $$\boldsymbol{\mu_n} = \boldsymbol{\Sigma_n} \left (\frac{1}{\sigma^2}\boldsymbol{X^Ty} \right)^{-1}.$$

### Posterior distribution for $\sigma^2$

Given $\boldsymbol{\beta}$, the posterior for $\sigma^2$ can be calculated as follows:\
$[\sigma^2|\boldsymbol{y,\beta}] \sim N(\boldsymbol{X\beta, \sigma^2I_n}) \times IG(a , b) \sim IG(a_n , b_n)$, where

* $a_n = \frac{n}{2} + a$ and
* $b_n = \frac{1}{2\sigma^2}\boldsymbol{(y - X\beta)^T(y -  X\beta)} + b$


## Implementing Gibbs Sampler for linear regression in R

```{r, echo = FALSE, warning=FALSE, message=FALSE}
source(here::here("R", "libraries.R"))
source(here::here("R", "convert-to-coda.R"))
source(here::here("R", "mcmc_lm.R"))
source(here::here("R", "adapt_mh_multi_lm.R"))
```

### Simulated data for model training and testing 

In this post we are using simulated data consisting of a matrix of covariates $\mathbf{X} = (\mathbf{x_1, \ldots, \mathbf{x_p}})'$, and predetermined parameters $\boldsymbol{\beta} = (\beta_0, \ldots, \beta_p)'$ and $\sigma^2$, where p represents the number covariates including the column of ones for the intercept $\beta_0$. 

```{r, echo = TRUE}
set.seed(111)

n = 100
p <- 5 
truesigma2 = 4
truebeta <- c(rmvn(1, rep(2, p), diag(20, nrow = p, ncol = p)))
X <- cbind(1, rmvn(n, rep(50, p-1), diag(1, nrow = p-1, ncol = p-1)))
y <- X %*% truebeta + rnorm(n, mean = 0,  sd = sqrt(truesigma2))

plot(y, X %*% truebeta, main = " Response vs expected response", 
     ylab = "expecte response", cex.lab = 1, cex.main=1)

```

### Run mcmc using Gibbs sampling algorithm

Running mcmc may take a long time depending on the the type of problem we are solving. To save us this time, we run it once and save the output in our results folder so that we will not have to rerun the mcmc, but we load the output instead.

The following example shows how to check if the mcmc output "out_lm" is not already save in the results folder, and if it is not there, call the function "mcmc_lm" to run mcmc using Gibbs sampling algorithm and then save the output as "out_lm" in the results folder.
 
```{r, echo = TRUE}

if(!file.exists(here::here("results", "out_lm.RData"))){
   
    out_lm <- mcmc_lm(niter = 10000) 
    save(out_lm,  file = here::here("results", "out_lm.RData"))
} else {
  
  load(here::here("results", "out_lm.RData"))
}

mean(out_lm[[2]])
```


### Check Gibbs samling mcmc performance

```{r, echo = FALSE, warning=FALSE, message=FALSE}
# burn_in <- seq(1:2000)
# beta_out <- out_lm[[1]][-burn_in, ]
# sigma_out <- out_lm[[2]][-burn_in]

# Rearranging the mcmc output to comfort with coda package

out_coda <- as.mcmc(convert_to_coda_single_chain(out_lm))
out_coda <- tidy_draws(out_coda) ## convert to tidybay
```

## Trace plots

A trace plot is a graph showing mcmc samples at each iteration. It provides a visual evaluation about convergence and mixing of the chain. As shown in the Figure \@ref(fig:trace-plot), chain mixes well and seems to have converged.

```{r trace-plot, fig.cap="Trace plot for model parameters", echo = FALSE, warning=FALSE}

out_coda %>%
    mcmc_trace(regex_pars = c("beta", "sigma2"),
              facet_args = list(nrow = 3,  ncol = 3))
```

The next plots show the parameter posterior distributions compared to the true parameters we used to simulate the data. This is one of the advantages of using simulated data, because it allows us to assess the performance of the model by comparing the true parameter to the estimated parameters.

```{r trace-plot1, fig.cap="Trace plot to compare mcmc sample to the true values. This plot does not include the intercept for a good visualization", echo = FALSE, warning=FALSE}

#colors <- c("red",  "blue", "green", "purple", "black", "orange")

par(mfrow = c(1,3))

plot(out_lm$sigma2, type = "l" , main = "True sigma2 vs estimated sigma2", cex.main=1)
abline(h = truesigma2, col = "red")

matplot(out_lm$beta[,-1], type = "l", xlab = "iterations", ylab = "beta values", 
        col = c("red",  "blue", "green", "purple"), 
        main = "True beta vs estimated beta", cex.main=1)
abline(h = truebeta[-1], col = c("red",  "blue", "green", "purple"))

plot(c(truebeta[-1], truesigma2), c(apply(out_lm[[1]][,-1], 2, mean), mean(out_lm[[2]])), ylab = "beta post mean", xlab = "true beta", main  = "true parameter vs posterior means", cex.main=1)
abline(0, 1, col = "red")

```

### Effective sample size

Effective sample size (ESS) can be defined as an estimate of the number of independent draws from the posterior distributions generated by the Markov chain runs. With the number of mcmc iterations N, the larger the ratio ESS/N, the more efficient the the chain will be. A very small value, commonly less that 0.1 indicates a very high autocorrelation among mcmc samples. Figure \@ref(fig:Eff-sample-size) shows that the effective sample size for this model, where the value of the ration is indicated by a dot at the end of the line.
The ESS greater than one, is commonly due to the negative autocorrelation between mcmc samples.

```{r Eff-sample-size, fig.cap="mcmc effective ratios", echo = FALSE, warning=FALSE}
 mcmc_neff(effectiveSize(out_coda)/10000)  

#effectiveSize(out_coda)/10000
```

# Metropoilis Hastings for linear regression

Using Metropolis-Hastings algorithm to sample from the joint posterior distribution using a Gaussian proposal distribution, has some advantages, weakens and challenges. The main advantage is that MH does not require the full conditional posterior distributions in their parametric form, which makes it work even if the priors are not conjugate with their posteriors. On the other hand, MH mcmc samples are accepted with probability less that one, because they come from the proposal distribution rather than being directly drawn from parameter distributions. In addition to the acceptance rate issue, the proposal distribution parameters should be chosen carefully for the chain to mix well, and it requires to work on log scale to avoid computational errors.

## Proposal distribution

The most common choice for proposal distribution is a normal distribution.
Because normal distributions are symmetric, with a Gaussian proposal distribution the acceptance ratio in \ref{eqn:accept-ratio} simplifies to

\begin{equation}
\label{eqn:accept-ratio-reduced}
\alpha = min\left(1,\frac{[\boldsymbol{\theta}^* | .]}{[\boldsymbol{\theta}^{(t)} | .]} \right).
\end{equation}


## Acceptance ration and mixing 

With a normal proposal distribution $[x|\mu, \sigma^2]$, the acceptance and rejection of the new sample mainly depends on the proposal variance $\sigma^2$. Figure \@ref(fig:tuning-choices) illustrates different behaviors 
in trace plots resulting from different choices of tuning parameters.

```{r tuning-choices, echo = FALSE, fig.cap="Trace plots resulting from different tuning parameters"}
knitr::include_graphics(here::here("figures", "compare-adap-non-adapt.png"))
```

 * If the proposal variance is too small, the acceptance rate will be high, because the proposed value and the current value are very close. In this case, the algorithm converge slowly and resulting mcmc samples will be highly correlated, which reduces the effective sample size.

 * If the proposal is too wide, the proposal is more likely to be rejected and the acceptance rate will be too small, because the algorithm makes big steps from the current state to the new one. If so, the algorithm may stay at the same state for many iterations, resulting in a a Markov chain with many similar samples, hence inefficient.

To obtain efficient samples using MH algorithm with a normal proposal, the proposal variance should neither be too small nor too high; but as shown in Figure \@ref(fig:tuning-choices), it is still difficult to figure out what the best tuning parameter would be. In this post we used an adaptive MH mcmc algorithm in which the tuning parameter updates automatically during the run of the algorithm, making the mcmc samples more efficient. For more on adaptive MH mcmc, see 
[Andrieu and Thoms, 2008](https://people.eecs.berkeley.edu/~jordan/sail/readings/andrieu-thoms.pdf)


## Working on log scale

We know by the definition of a likelihood function that it involves a product of many probabilities that may include values very close to zero. Such a product results in a very small number that computers will usually round to zero. Therefore, computing expressions containing the likelihood function as a denominator or a product involving a likelihood function as a denominator, causes a numerical error. Foe example, the denominator in acceptance probability in Equation \ref{eqn:accept-ratio}. Working on logarithm scale transforms the likelihood into a sum leading, which resolves the issue.

## Fiitting a multiple linear regression using MH algorithm in R


```{r, echo = TRUE, message=FALSE, warning=FALSE}

#set.seed(111)

# load MH mcmc model output

if(!file.exists(here::here("results", "adapt__mh_lm_out10.RData"))){
   
Sig_chol <- matrix(0, p+1, p+1)
Sig_chol[lower.tri(Sig_chol, diag = TRUE)] <- 1

adapt__mh_lm_out10 <- Adapt_MH_MCMC_lm(X = X, y = y, K = 10000, Sigma_tune = diag(1, p+1), Sig_chol, update_tune = TRUE)
    save(adapt__mh_lm_out10 ,  file = here::here("results", "adapt__mh_lm_out10.RData"))
    
} else {
  
  load(here::here("results", "adapt__mh_lm_out10.RData"))
}
matplot(adapt__mh_lm_out10,  type = "l")
```

## Check MH mcmc performance

<!-- # ```{r, echo = FALSE, warning=FALSE, message=FALSE} -->
<!-- # out_coda_mh <- convert_to_coda_single_chain(adapt__mh_lm_out10) -->
<!-- # out_coda_mh <- tidy_draws(out_coda_mh) ## convert to tidybay -->
<!-- # ``` -->

### Trace plots

```{r trace-plot-mh, fig.cap ="Trace plot for MH mcmc model parameters", echo = FALSE, warning=FALSE}

matplot(adapt__mh_lm_out10, type = "l",  xlab = "iterations", col = c("red",  "blue", "green", "purple", "black", "orange"), main = "true beta vs estimated beta")
abline(h = c(truebeta, truesigma2), col = c("red",  "blue", "green", "purple", "black", "orange"))
```

<!-- ### Acceptance ratio -->

<!-- ```{r accept-ratio, fig.cap="acceptance ratio in MH sampling",  echo = FALSE, warning=FALSE} -->

<!-- acceptance <- out_lm_mh$a -->
<!-- accepted_samples <- length(acceptance[acceptance[TRUE]]) -->
<!-- head(accepted_samples) -->

<!-- accept_ratio <- 100*(accepted_samples/niter) -->
<!-- accept_ratio -->

<!-- # knitr::kable(head(accepted_samples)) -->
<!-- ``` -->

### Effective sample size

```{r Eff-sample-size_mh, fig.cap="mcmc effective ratios", echo = FALSE, warning=FALSE}
 # mcmc_neff(effectiveSize(out_coda_mh) / niter)
effectiveSize(adapt__mh_lm_out10) / 10000
```

## Comparing Regression model diagnostics between Gibbs and MH mcmc

### Gibbs mcmc regression model diagnostics


```{r Gibbs-model-diag, fig.cap = "Gibbs mcmc regression model diagnostics", echo = FALSE, warning=FALSE, message=FALSE}
beta_post_mean <- apply(out_lm$beta, 2, mean) # Posterior mean of beta

y_pred <- X %*% beta_post_mean

residuals <- y - y_pred

# Fitted density vs true density
#plot(density(y_pred), col = "red", main = "Fitted density vs observed density", cex.main=1)
plot(density(y), col = "red", main = "Fitted density vs observed density", cex.main=1)
lines(density(y_pred), col = "blue")
legend("topright", legend = c("true density",  "fitted density"),
       col = c("red", "blue"), lty=1:2, cex=0.8)

# Residual and fitted values plots

par(mfrow = c(2,2))
hist(residuals, main = "Residual histogram",  cex.main=1)
qqnorm(residuals, main = "Residual qqplot", cex.main=1)
qqline(residuals, col = "red")
plot( y_pred, residuals, main = "Resuduals vs fitted values", cex.main=1)
plot(y, y_pred, main = "Observed values vs predicted values", cex.main=1)
abline(c(0, 1), col ="blue") ## a line of 45 degree angle
```

### MH regression model diagnostics $iterations = 10,000$

```{r MH-model-diag10, fig.cap = "10000 iteratons MH mcmc regression model diagnostics", echo = FALSE, warning=FALSE, message=FALSE}
beta_post_mean_mh10 <- apply(adapt__mh_lm_out10[, 1:p], 2, mean) # Posterior mean of beta

y_pred_mh10 <- X %*% beta_post_mean_mh10

resid_mh10 <- y - y_pred_mh10

# Fitted density vs true density

plot(density(y), col = "red", main = "Fitted density vs observed density", cex.main=1)
lines(density(y_pred_mh10), col = "blue")
legend("topright", legend = c("true density",  "fitted density"),
       col = c("red", "blue"), lty=1:2, cex=0.8)

# Residual and fitted values plots

par(mfrow = c(2,2))
hist(resid_mh10, main = "residual histogram")
qqnorm(resid_mh10, main = "residual qqplot")
qqline(resid_mh10, col = "red")
plot(resid_mh10, y_pred_mh10, main = "resuduals vs fitted values")
plot(y, y_pred_mh10, main = "Observed values vs predicted values")
abline(c(0, 1), col ="blue") ## a line of 45 degree angle
```

Consider $N = 100,000$ 

```{r, echo = TRUE, message=FALSE, warning=FALSE}

#set.seed(111)

# load MH mcmc model output

if(!file.exists(here::here("results", "adapt__mh_lm_out100.RData"))){
   
    adapt__mh_lm_out100 <- Adapt_MH_MCMC_lm(X = X, y = y, K = 100000, 
                                     Sigma_tune = diag(1, p+1), Sig_chol, update_tune = TRUE)
    save(adapt__mh_lm_out100 ,  file = here::here("results", "adapt__mh_lm_out100.RData"))
    
} else {
  
  load(here::here("results", "adapt__mh_lm_out100.RData"))
}


matplot(adapt__mh_lm_out100, type = "l")
```


```{r MH-model-diag100, fig.cap = "100,000 iterations MH mcmc regression model diagnostics", echo = FALSE, warning=FALSE, message=FALSE}

beta_post_mean_mh100 <- apply(adapt__mh_lm_out100[, 1:p], 2, mean) # Posterior mean of beta

y_pred_mh100 <- X %*% beta_post_mean_mh100

resid_mh100 <- y - y_pred_mh100

# Fitted density vs true density

plot(density(y), col = "red", main = "Fitted density vs observed density")
lines(density(y_pred_mh100), col = "blue")
legend("topright", legend = c("true density",  "fitted density"),
       col = c("red", "blue"), lty=1:2, cex=0.8)

# Residual and fitted values plots

par(mfrow = c(2,2))
hist(resid_mh100, main = "residual histogram")
qqnorm(resid_mh100, main = "residual qqplot")
qqline(resid_mh100, col = "red")
plot(resid_mh100, y_pred_mh100, main = "resuduals vs fitted values")
plot(y, y_pred_mh100, main = "Observed values vs predicted values")
abline(c(0, 1), col ="blue") ## a line of 45 degree angle
```
